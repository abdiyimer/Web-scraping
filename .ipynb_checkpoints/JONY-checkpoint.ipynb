{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1327fabf-1a85-46a7-9ca3-9a6297036e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "class GITHUB(scrapy.Spider):\n",
    "    name ='Github_Crawler'\n",
    "    def start_requests(self):\n",
    "        url ='https://github.com/topics'\n",
    "        yield scrapy.Request(url,callback=self.parse)\n",
    "    def parse(self,response):\n",
    "        links=[]\n",
    "        for a in response.css('a'):\n",
    "            link = a.attrib.get('href')\n",
    "            if link and \"/topics/\" in link:\n",
    "                links.append(link)   \n",
    "                \n",
    "        #yield {\"Links to all the topics \":links}   \n",
    "        links_concat=[]\n",
    "        for a in links:\n",
    "            link = \"https://github.com\"+a\n",
    "            links_concat.append(link)\n",
    "        for url in links_concat:\n",
    "            yield response.follow(url,self.parse1)\n",
    "\n",
    "    def parse1(self, response):\n",
    "        Articles = []\n",
    "     \n",
    "        article = response.css('div.markdown-body.f5.mb-2 p::text').getall()\n",
    "        name=response.css('h1.h1::text').getall()\n",
    "        name = ''.join(name).strip()\n",
    "        \n",
    "        \n",
    "        link='https://github.com'+name\n",
    "\n",
    "        yield {\"Topic\":name,\"Description\":article,\"url\":link}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed0dd63-b822-46f7-9583-5681a5fda9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 16:30:06 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: scrapybot)\n",
      "2024-04-14 16:30:06 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.2.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.10.0, Python 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 23.2.0 (OpenSSL 3.0.13 30 Jan 2024), cryptography 41.0.3, Platform Windows-10-10.0.22631-SP0\n",
      "2024-04-14 16:30:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2024-04-14 16:30:06 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-04-14 16:30:06 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-04-14 16:30:06 [scrapy.extensions.telnet] INFO: Telnet Password: fbf20ac3f453b8b9\n",
      "2024-04-14 16:30:06 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\extensions\\feedexport.py:315: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2024-04-14 16:30:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-04-14 16:30:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-04-14 16:30:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-04-14 16:30:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-04-14 16:30:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-04-14 16:30:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-04-14 16:30:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-04-14 16:30:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics> (referer: None)\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://github.com/topics/3d> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:07 [py.warnings] WARNING: C:\\Users\\asus\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py:306: RuntimeWarning: Could not load referrer policy 'origin-when-cross-origin, strict-origin-when-cross-origin'\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "2024-04-14 16:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/pico-8> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/pico-8>\n",
      "{'Topic': 'PICO-8', 'Description': ['PICO-8 is a fantasy console for making, sharing and playing tiny games and other computer programs. When you turn it on, the machine greets you with a shell for typing in Lua commands and provides simple built-in tools for creating your own cartridges.'], 'url': 'https://github.comPICO-8'}\n",
      "2024-04-14 16:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/3d> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/ajax> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/ansible> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/amphp> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/3d>\n",
      "{'Topic': '3D', 'Description': ['3D technology is used in a wide range of fields, including film, video games, architecture, engineering, and product design. It allows designers to create digital models of objects that can be manipulated and rendered in three dimensions. 3D modeling software is used to create and manipulate 3D models, and 3D animation software is used to create movement and effects within those models. 3D technology has also been adopted for use in 3D printing, where physical objects can be created from digital models.'], 'url': 'https://github.com3D'}\n",
      "2024-04-14 16:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/algorithm> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/ajax>\n",
      "{'Topic': 'Ajax', 'Description': ['Ajax stands for asynchronous JavaScript and XML. It is collection of several web technologies including HTML, CSS, JSON, XML, and JavaScript. It is used for creating dynamic web pages in which small parts of web page change without reloading the page. Additionally, Ajax is fundamental for front-end developers in creating Single Page Applications (SPAs) where content updates seamlessly.'], 'url': 'https://github.comAjax'}\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/ansible>\n",
      "{'Topic': 'Ansible', 'Description': ['Ansible is a simple and powerful automation engine. It is used to help with configuration management, application deployment, and task automation.'], 'url': 'https://github.comAnsible'}\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/amphp>\n",
      "{'Topic': 'Amp', 'Description': [\"Amp is a non-blocking concurrency library for PHP. Newer versions make use of fibers to integrate well with existing interfaces and allow for transparent non-blocking I/O. Older versions make heavy use of generator-based coroutines. It's made by the developers that brought native fiber support to PHP 8.1.\"], 'url': 'https://github.comAmp'}\n",
      "2024-04-14 16:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/docker> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/swift> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/algorithm>\n",
      "{'Topic': 'Algorithm', 'Description': ['Algorithms are detailed sets of guidelines created for a computer program to complete tasks efficiently and thoroughly. Algorithms in computer programming are employed to solve complex problems. Various cutting-edge technologies including artificial intelligence (AI) and machine learning (ML), operate based on simple or complex algorithms.'], 'url': 'https://github.comAlgorithm'}\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/docker>\n",
      "{'Topic': 'Docker', 'Description': [' is software that provides containers, which allows teams to emulate development environments. It began as an internal project, initially developed by dotCloud engineers.'], 'url': 'https://github.comDocker'}\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/swift>\n",
      "{'Topic': 'Swift', 'Description': ['Swift is a general-purpose programming language built using a modern approach to safety, performance, and software design patterns. Inspired by many other modern programming languages, Swift is intended to be more resilient and expressive than Objective-C, its predecessor.'], 'url': 'https://github.comSwift'}\n",
      "2024-04-14 16:30:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/angular> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:09 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/angular>\n",
      "{'Topic': 'Angular', 'Description': [\"Released in 2016, Angular is a rewrite of AngularJS. It focuses on good mobile development, modularity, and improved dependency injection. Angular is designed to comprehensively address a developer's web application workflow.\"], 'url': 'https://github.comAngular'}\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/arduino> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/android> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/aws> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/api> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/aspnet> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/atom> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/arduino>\n",
      "{'Topic': 'Arduino', 'Description': ['Arduino is an open source platform that allows users to easily build and program electronic devices using a variety of hardware and software tools. It is popular among hobbyists, educators, and professionals for its versatility and ease of use. With a variety of available microcontrollers, sensors, actuators, and other components, Arduino enables users to create a wide range of projects, from simple circuits to complex systems. Whether you are just getting started with electronics or have experience building projects, Arduino has something to offer for everyone.'], 'url': 'https://github.comArduino'}\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/awesome> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/android>\n",
      "{'Topic': 'Android', 'Description': ['Android was designed and built by Google in 2008. The operating system is written mainly in Java, with core components in C and C++. It is built on top of the Linux kernel, giving it incorporated security benefits.'], 'url': 'https://github.comAndroid'}\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/aws>\n",
      "{'Topic': 'Amazon Web Services', 'Description': ['Amazon Web Services (AWS) is a subsidiary of Amazon.com that provides on-demand cloud computing platforms to individuals, companies, and governments, on a subscription basis. Compute, storage, database, networking, security, management & developer tools, AI & machine learning, analytics, etc. are some of the primary aspects of AWS.'], 'url': 'https://github.comAmazon Web Services'}\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/api>\n",
      "{'Topic': 'API', 'Description': ['An ', ' can be thought of as an instruction manual for communication between multiple software apparatuses. For example, an API may be used for database communication between web applications. By\\xa0extracting\\xa0the implementation and relinquishing data into objects, an API simplifies programming.'], 'url': 'https://github.comAPI'}\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/aspnet>\n",
      "{'Topic': 'ASP.NET', 'Description': ['ASP.NET is an open source web framework for building modern web apps and services with .NET. ASP.NET creates websites based on HTML5, CSS, and JavaScript that are simple, fast, and can scale to millions of users.'], 'url': 'https://github.comASP.NET'}\n",
      "2024-04-14 16:30:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/ansible-code-bot-scan> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/atom>\n",
      "{'Topic': 'Atom', 'Description': ['Atom is a modern open source text editor developed by GitHub. It is designed to be approachable out of the box yet highly customizable. Atom is built using web technologies: the look and feel can be customized using CSS and new features can be added with HTML and JavaScript. There are also thousands of community created themes and packages available.'], 'url': 'https://github.comAtom'}\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/awesome>\n",
      "{'Topic': 'Awesome Lists', 'Description': ['An awesome list is a list of awesome things curated by the community. There are awesome lists about everything from ', ' to ', '. The ', ' serves as a curated list of awesome lists.'], 'url': 'https://github.comAwesome Lists'}\n",
      "2024-04-14 16:30:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/ansible-code-bot-scan>\n",
      "{'Topic': 'ansible-code-bot-scan', 'Description': [], 'url': 'https://github.comansible-code-bot-scan'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/github-config> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/html> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/github-config>\n",
      "{'Topic': 'github-config', 'Description': [], 'url': 'https://github.comgithub-config'}\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/html>\n",
      "{'Topic': 'HTML', 'Description': ['HTML, or Hypertext Markup Language, was primarily designed to provide a means of creating structured scientific documents.\\xa0HTML can embed scripting languages such as PHP or JavaScript to affect the behavior and content of web pages. The\\xa0World Wide Web Consortium\\xa0(W3C) maintains both the HTML and CSS standards.'], 'url': 'https://github.comHTML'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/python> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/python>\n",
      "{'Topic': 'Python', 'Description': ['Python is a dynamically-typed garbage-collected programming language developed by Guido van Rossum in the late 80s to replace ABC. Much like the programming language Ruby, Python was designed to be easily read by programmers. Because of its large following and many libraries, Python can be implemented and used to do anything from webpages to scientific research.'], 'url': 'https://github.comPython'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/css> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/python3> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/config> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/css>\n",
      "{'Topic': 'CSS', 'Description': ['Cascading Style Sheets (CSS) is a language used most often to style and improve upon the appearance of websites. It allows for the separation of presentation and content, and includes the characteristics of layouts, colors and fonts. CSS builds upon HTML to make webpages more interactive and appealing to the user.'], 'url': 'https://github.comCSS'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/javascript> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/react> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/python3>\n",
      "{'Topic': 'python3', 'Description': ['Python is a dynamically-typed garbage-collected programming language developed by Guido van Rossum in the late 80s to replace ABC. Much like the programming language Ruby, Python was designed to be easily read by programmers. Because of its large following and many libraries, Python can be implemented and used to do anything from webpages to scientific research.'], 'url': 'https://github.compython3'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/cpp> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/config>\n",
      "{'Topic': 'config', 'Description': [], 'url': 'https://github.comconfig'}\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/javascript>\n",
      "{'Topic': 'JavaScript', 'Description': ['JavaScript (JS) is a lightweight interpreted or JIT-compiled programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, multi-paradigm, dynamic language, supporting object-oriented, imperative, and declarative (e.g. functional programming) styles.'], 'url': 'https://github.comJavaScript'}\n",
      "2024-04-14 16:30:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/react>\n",
      "{'Topic': 'React', 'Description': ['React (also known as React.js or ReactJS) is a JavaScript library that makes developing interactive user interfaces simple.'], 'url': 'https://github.comReact'}\n",
      "2024-04-14 16:30:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/covid-19> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/cpp>\n",
      "{'Topic': 'C++', 'Description': ['C++ is a popular and widely used mid-level language. It was designed as an extension of the C language.'], 'url': 'https://github.comC++'}\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/covid-19>\n",
      "{'Topic': 'COVID-19', 'Description': ['The coronavirus disease 2019 (COVID-19) is an infectious disease caused by a type of coronavirus known as SARS-CoV-2 that had caused an ongoing pandemic. This topic is associated with repositories that contain code focused around research and awareness of the virus.'], 'url': 'https://github.comCOVID-19'}\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/continuous-integration> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/continuous-integration>\n",
      "{'Topic': 'Continuous integration', 'Description': ['Automatically build and test your code as you push it upstream, preventing bugs from being deployed to production. A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests. Integration tests are usually run automatically on a CI server when it detects a new commit.'], 'url': 'https://github.comContinuous integration'}\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/code-quality> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/code-review> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/compiler> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/clojure> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/chrome-extension> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/code-quality>\n",
      "{'Topic': 'Code quality', 'Description': ['Automate your code review with style, quality, security, and testâ€‘coverage checks when you need them most. Code quality is intended to keep complexity down and runtime up.'], 'url': 'https://github.comCode quality'}\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/code-review>\n",
      "{'Topic': 'Code review', 'Description': ['Code review is systematic examination (sometimes referred to as peer review) of computer source code. It is intended to find mistakes overlooked in software development, improving the overall quality of software. Reviews are done in various forms such as pair programming, informal walkthroughs, and formal inspections.'], 'url': 'https://github.comCode review'}\n",
      "2024-04-14 16:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/cli> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/compiler>\n",
      "{'Topic': 'Compiler', 'Description': ['Compilers are software that translate higher-level (more human readable) programming languages to lower-level languages (e.g. machine code). The processor executes machine code, which indicates when binary high and low signals are required in the arithmetic logic unit of the processor. Examples of compiled languages include BASIC, Fortran, C++, C, and Java.'], 'url': 'https://github.comCompiler'}\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/clojure>\n",
      "{'Topic': 'Clojure', 'Description': ['Clojure is a functional, dynamic, general-purpose programming language. It provides built-in concurrency support via software transactional memory and asynchronous agents, and offers a rich set of immutable, persistent data structures. Clojure runs on JVM, JavaScript VMs, and Common Language Runtime.'], 'url': 'https://github.comClojure'}\n",
      "2024-04-14 16:30:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/chrome-extension>\n",
      "{'Topic': 'Chrome extension', 'Description': ['Chrome extensions are add-ons for Chromium-based browsers, such as Google Chrome, which enable users to customize the Chrome browsing experience. Built using web technologies like HTML, CSS, and JavaScript, extensions make use of various APIs supported by the browser. Extensions are typically downloaded from online marketplaces, with the Chrome Web Store being the most widely used. Chrome extensions work on most Chromium browsers, not just Google Chrome developed by Google.'], 'url': 'https://github.comChrome extension'}\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/cli>\n",
      "{'Topic': 'Command line interface', 'Description': ['Before there were graphical user interfaces, command-line interfaces were used to issue commands to a computer. Programs that handle the user interface are called command language interpreters, often known as a shell. A CLI may give a user more control over the computer and programs they wish to execute.'], 'url': 'https://github.comCommand line interface'}\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/chrome> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/c> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/chrome>\n",
      "{'Topic': 'Chrome', 'Description': [\"Chrome is the most popular web browser worldwide as of mid-2017, made by the tech company Google. It's available for most operating systems including Windows, macOS, and Linux and on multiple platforms such as the desktop, phones, and tablets.\", 'Chrome boasts a minimalistic UI and was the first browser to feature \"tabs\" above the address bar, a convention that was later implemented in other browsers. Other popular features include things such as Incognito mode, tab sandboxing, and a Web Store with extensions and themes.', 'Although Chrome is not open source, the majority of the source code is available under the Chromium moniker.'], 'url': 'https://github.comChrome'}\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/c>\n",
      "{'Topic': 'C', 'Description': ['C is a programming language designed by Dennis Ritchie at Bell Labs. C is very widely used, straightforward, and can be compiled to a number of platforms and operating systems. C is an imperative language, with a small number of keywords and a large number of mathematical operators. C is also a very low level programming language, which means it can communicate directly with hardware.'], 'url': 'https://github.comC'}\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/bot> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/bitcoin> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/bootstrap> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/bot>\n",
      "{'Topic': 'Bot', 'Description': ['A bot is an application that runs automated, usually repetitive tasks over the Internet.'], 'url': 'https://github.comBot'}\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/bitcoin>\n",
      "{'Topic': 'Bitcoin', 'Description': ['Bitcoin is a cryptocurrency developed by Satoshi Nakamoto in 2009. Bitcoin is used as a digital payment system. Rather than use traditional currency (USD, YEN, EURO, etc.) individuals may trade in, or even mine Bitcoin. It is a peer-to-peer system, and transactions may take place between users directly.'], 'url': 'https://github.comBitcoin'}\n",
      "2024-04-14 16:30:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/bootstrap>\n",
      "{'Topic': 'Bootstrap', 'Description': [' is a popular front-end framework that streamlines website design. It allows for the creation of easy and responsive web layouts.'], 'url': 'https://github.comBootstrap'}\n",
      "2024-04-14 16:30:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/azure> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/bash> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/azure>\n",
      "{'Topic': 'Azure', 'Description': ['Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through a global network of Microsoft-managed data centers.'], 'url': 'https://github.comAzure'}\n",
      "2024-04-14 16:30:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/bash>\n",
      "{'Topic': 'Bash', 'Description': ['Bash (Bourne Again Shell) is a shell and command language interpreter for the GNU / Linux operating systems. It is meant to be an improved version of Bourne Shell. It is used as a default login shell for most Linux distributions. Bash can read and execute shell script (.sh) files to automate the execution of tasks.'], 'url': 'https://github.comBash'}\n",
      "2024-04-14 16:30:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://github.com/topics/babel> (referer: https://github.com/topics)\n",
      "2024-04-14 16:30:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://github.com/topics/babel>\n",
      "{'Topic': 'Babel', 'Description': ['ðŸ  Babel is a compiler created by ', ' in 2014 to convert ES6 to ES5 (originally called 6to5). It has since become a toolchain that enables developers to write any next generation JavaScript and serves as a testing ground for proposals from ', ', the technical committee that specifies ', '. Babel can also convert ', ' and strip out type annotations from both ', ' and ', '. Babel is built out of plugins. Compose your own transformation pipeline using ', ' or ', '.'], 'url': 'https://github.comBabel'}\n",
      "2024-04-14 16:30:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-04-14 16:30:14 [scrapy.extensions.feedexport] INFO: Stored csv feed (42 items) in: New_Scrapped_Data.csv\n",
      "2024-04-14 16:30:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 29163,\n",
      " 'downloader/request_count': 43,\n",
      " 'downloader/request_method_count/GET': 43,\n",
      " 'downloader/response_bytes': 2641794,\n",
      " 'downloader/response_count': 43,\n",
      " 'downloader/response_status_count/200': 43,\n",
      " 'dupefilter/filtered': 31,\n",
      " 'elapsed_time_seconds': 7.724221,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 4, 14, 15, 30, 14, 675839),\n",
      " 'httpcompression/response_bytes': 20523307,\n",
      " 'httpcompression/response_count': 43,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/DEBUG': 87,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 75,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 43,\n",
      " 'scheduler/dequeued': 43,\n",
      " 'scheduler/dequeued/memory': 43,\n",
      " 'scheduler/enqueued': 43,\n",
      " 'scheduler/enqueued/memory': 43,\n",
      " 'start_time': datetime.datetime(2024, 4, 14, 15, 30, 6, 951618)}\n",
      "2024-04-14 16:30:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "    'FEED_FORMAT': 'csv',  # Output format is CSV\n",
    "    'FEED_URI': 'New_Scrapped_Data.csv'}  # Save the output to a CSV file\n",
    "    \n",
    ")\n",
    "process.crawl(GITHUB)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5740e80d-4ff5-4b67-8c01-8cadea605092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\asus\\\\Desktop\\\\jony\\\\jony'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "topics_url = 'https://github.com/topics'\n",
    "response = requests.get(topics_url)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d698ede-f0c6-4db9-9360-a9323861a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e35c2a7-bd22-4f8b-9afa-913921907ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "doc = BeautifulSoup(page_contents, 'html.parser')\n",
    "selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "\n",
    "len(topic_title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b652e9b-c6cf-4fb5-bd3f-848b8144ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "topic_desc_tags = doc.find_all('p', {'class': desc_selector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9511e025-5eb3-4010-b3b2-fa435c58704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"no-underline flex-1 d-flex flex-column\" href=\"/topics/3d\">\n",
       "<p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">3D</p>\n",
       "<p class=\"f5 color-fg-muted mb-0 mt-1\">\n",
       "          3D refers to the use of three-dimensional graphics, modeling, and animation in various industries.\n",
       "        </p>\n",
       "</a>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_title_tag0 = topic_title_tags[0]\n",
    "div_tag = topic_title_tag0.parent\n",
    "topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "len(topic_link_tags)\n",
    "topic_link_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1193469-fbff-4f3b-9a64-2f1a77dd08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/topics/3d\n"
     ]
    }
   ],
   "source": [
    "topic0_url = \"https://github.com\" + topic_link_tags[0]['href']\n",
    "print(topic0_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66ced4a-3d81-483d-b73c-b6babcadc82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android', 'Angular', 'Ansible', 'API', 'Arduino', 'ASP.NET', 'Atom', 'Awesome Lists', 'Amazon Web Services', 'Azure', 'Babel', 'Bash', 'Bitcoin', 'Bootstrap', 'Bot', 'C', 'Chrome', 'Chrome extension', 'Command line interface', 'Clojure', 'Code quality', 'Code review', 'Compiler', 'Continuous integration', 'COVID-19', 'C++']\n"
     ]
    }
   ],
   "source": [
    "topic_titles = []\n",
    "\n",
    "for tag in topic_title_tags:\n",
    "    topic_titles.append(tag.text)\n",
    "    \n",
    "print(topic_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02702a3-1987-4b56-913b-7cfd57f9f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_descs = []\n",
    "\n",
    "for tag in topic_desc_tags:\n",
    "    topic_descs.append(tag.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d0bb0e-dbc0-47f4-b707-bf3d416d5a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_urls = []\n",
    "base_url = 'https://github.com'\n",
    "\n",
    "for tag in topic_link_tags:\n",
    "    topic_urls.append(base_url + tag['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3f8acc-a000-4f5f-af28-1b39290324c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topics_dict = {\n",
    "    'title': topic_titles,\n",
    "    'description': topic_descs,\n",
    "    'url': topic_urls\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792b7bef-2840-41fc-9438-9fac648d8d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_page_url = topic_urls[0]\n",
    "topic_page_url\n",
    "'https://github.com/topics/3d'\n",
    "response = requests.get(topic_page_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9cd93f2-73b5-4e8f-9f06-de754bd4e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5f4935-d343-4030-b4e7-5810ed5ecbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class})\n",
    "\n",
    "owner = repo_tags[0].find('a').text.strip()\n",
    "repo_name = repo_tags[0].find_all('a')[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "859aaa84-46a1-4af9-8b5b-517255ccefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = repo_tags[0].find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc3cdb0b-37fb-46da-8806-5691be4a279f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrdoob'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42cca878-d600-4b37-b2ba-3d9cdf5f5875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/mrdoob/three.js'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://github.com'\n",
    "repo_url = base_url + a_tags[1]['href']\n",
    "repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac732c2e-8f20-46aa-bac4-6f3d6d4d31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tags = topic_doc.find_all('span', { 'class': 'Counter js-social-count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "351ed960-b8c7-4ab6-8a35-700374d1d42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22.6k'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_tags[2].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b12a45b-204b-44f5-bacf-dd8c39b0ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14e66bb3-427f-4bef-9522-1469a3ef0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff7c911a-f445-46ef-9566-2b8d195f6b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mrdoob', 'three.js', 22600, 'https://github.com/mrdoob/three.js')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_info(repo_tags[0], star_tags[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92f06074-6b50-4b83-ba46-c9340324fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_repos_dict = {\n",
    "    'username': [],\n",
    "    'repo_name': [],\n",
    "    'stars': [],\n",
    "    'repo_url': []\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(repo_tags)):\n",
    "    repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "    topic_repos_dict['username'].append(repo_info[0])\n",
    "    topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "    topic_repos_dict['stars'].append(repo_info[2])\n",
    "    topic_repos_dict['repo_url'].append(repo_info[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7d98565-01aa-469c-8df0-ccfa460e138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc\n",
    "\n",
    "def get_repo_info(h1_tag, star_tag):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = h1_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "    # Get the h1 tags containing repo title, repo URL and username\n",
    "    h1_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class})\n",
    "    # Get star tags\n",
    "    star_tags = topic_doc.find_all('span', { 'class': 'Counter js-social-count'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
    "\n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9980e87-5c41-4fde-9079-70d783f99149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls\n",
    "    \n",
    "\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ca041a0-c45d-48a5-9d2e-5588b52aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b38d4e3-cdf4-4a69-8875-62825e6b743e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:01 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:02 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:02 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"3D\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:03 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/3d HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:03 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Ajax\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:05 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/ajax HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:05 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Algorithm\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:07 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/algorithm HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:07 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Amp\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:08 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/amphp HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:09 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Android\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:11 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/android HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:11 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Angular\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:12 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/angular HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:12 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Ansible\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:14 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/ansible HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:14 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"API\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:16 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/api HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:16 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Arduino\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:17 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/arduino HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:18 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"ASP.NET\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:19 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/aspnet HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:19 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Atom\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:21 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/atom HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:21 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Awesome Lists\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:22 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/awesome HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:23 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Amazon Web Services\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:24 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/aws HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:24 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Azure\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:26 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/azure HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:26 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Babel\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:27 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/babel HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:28 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Bash\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:29 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/bash HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:30 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Bitcoin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:31 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/bitcoin HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:31 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Bootstrap\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:32 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/bootstrap HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:33 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Bot\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:34 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/bot HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:34 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"C\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:36 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/c HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:37 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Chrome\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:38 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/chrome HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:38 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Chrome extension\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:40 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/chrome-extension HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:41 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Command line interface\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:42 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/cli HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:42 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Clojure\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:43 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/clojure HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:44 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Code quality\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:45 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/code-quality HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:45 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Code review\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:47 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/code-review HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:47 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Compiler\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:48 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/compiler HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:49 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"Continuous integration\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:50 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/continuous-integration HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:51 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"COVID-19\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:52 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/covid-19 HTTP/1.1\" 200 None\n",
      "2024-04-14 15:05:52 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): github.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"C++\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 15:05:54 [urllib3.connectionpool] DEBUG: https://github.com:443 \"GET /topics/cpp HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
